{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1653107503975,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"},"user_tz":-420},"id":"UJGL0qzsah8u","outputId":"b78f58b6-94c4-46ef-fc6b-de5215498142"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat May 21 04:31:43 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"qy1rG8X53sPF","executionInfo":{"status":"ok","timestamp":1653107503976,"user_tz":-420,"elapsed":7,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6576,"status":"ok","timestamp":1653107510546,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"},"user_tz":-420},"id":"EtEk6V7iOfhd"},"outputs":[],"source":["!pip install -q transformers\n","!pip install -q datasets"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3616,"status":"ok","timestamp":1653107514158,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"},"user_tz":-420},"id":"zar7AVFcOzfS"},"outputs":[],"source":["#transformers\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import RandomOverSampler \n","import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import Trainer, TrainingArguments"]},{"cell_type":"code","source":["tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","# model = RobertaForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2\", num_labels=3)"],"metadata":{"id":"SoFp7KcAh1aX","executionInfo":{"status":"ok","timestamp":1653107516425,"user_tz":-420,"elapsed":2281,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ciWQXgp0SAD3"},"source":["# Load Dataset\n"]},{"cell_type":"code","source":["train_dataset_path = \"/content/drive/MyDrive/NLP/Setiment_Analysis/en/dataset/covid-19/aug_clean_train.csv\"\n","test_dataset_path = \"/content/drive/MyDrive/NLP/Setiment_Analysis/en/dataset/covid-19/clean_test.csv\""],"metadata":{"id":"RhWshjVggda9","executionInfo":{"status":"ok","timestamp":1653107516427,"user_tz":-420,"elapsed":17,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset, load_dataset\n","\n","dataset = load_dataset('csv', data_files={'train': train_dataset_path,\n","                                          'val': test_dataset_path})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["9ed850e334534d119516ff2fd6670436","c4739ab6b5c04efc8dd8b8694b5db9a8","a09ae2fea20442f0a52bea2762b9279c","adc9f1f21fd54845b9f9ae2afdf52446","bf96e5420b5b4f62a5d116cabe640b14","b7864736a87241e8a60d5a60b8306d9f","82e2b9586ac7407bb49cc62dd33e085f","0b96a32ebb13430cafda10eb8d66aac0","d25cc6ca2486479f8d4645579309c50e","dc5f53b1fd284d5ab606b7f53caaa7f5","cc1fc4d14eac4b52a5ffd1c1ad19c39d"]},"id":"pWE5R3ylesdm","executionInfo":{"status":"ok","timestamp":1653107517136,"user_tz":-420,"elapsed":724,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}},"outputId":"e21267b7-f52e-4d72-a388-f77f2baea330"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Using custom data configuration default-e892ee5e9612a018\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-e892ee5e9612a018/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ed850e334534d119516ff2fd6670436"}},"metadata":{}}]},{"cell_type":"markdown","source":["## Preprocess data"],"metadata":{"id":"9sgbjZx9o6G2"}},{"cell_type":"code","source":["dataset = dataset.rename_column(\"text_clean\", \"texts\")\n","dataset = dataset.rename_column(\"Sentiment\", \"labels\")\n","# dataset['train'], dataset['validation'] = dataset['train'].train_test_split(0.2).values()"],"metadata":{"id":"t5htSYiRjY8l","executionInfo":{"status":"ok","timestamp":1653107517137,"user_tz":-420,"elapsed":16,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def label_mapping(examples):\n","    label_to_idx = {'Extremely Negative': 0, 'Negative': 0, 'Neutral': 1,\n","                    'Positive': 2, 'Extremely Positive': 2}\n","    examples[\"labels\"] = label_to_idx[examples[\"labels\"]]\n","    return examples\n","    \n","dataset = dataset.map(label_mapping)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MT_CQVthj75v","executionInfo":{"status":"ok","timestamp":1653107517138,"user_tz":-420,"elapsed":15,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}},"outputId":"a6659233-6685-45ec-9a93-f66aed199b45"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e892ee5e9612a018/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-31d171c191eea004.arrow\n","Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e892ee5e9612a018/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-45e6e6fe2b600af6.arrow\n"]}]},{"cell_type":"code","source":["def tokenizer_function(examples):\n","    result = tokenizer(examples[\"texts\"], max_length=128,\n","                       padding='max_length', truncation=True)\n","    result[\"labels\"] = examples[\"labels\"]\n","    return result\n","\n","tokenized_datasets = dataset.map(tokenizer_function, batched=True, remove_columns=['texts', 'labels'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["6d95761434164461a71a00c4c6e7a632","c533aa653a474d698ec9732e2e30c2f3","70cb991b03ce47578b17656d88c9f1d6","046c29f93b5b463f9ea7f0380e2ae8fe","c27bd27b1bd54160bb8142098b5a542a","d6b1df7a8a55447791983b0b0d7ed3f9","331108bd35cf4954a671fda40bcae4cd","c098c882ba374c0cb3bf1f3901e3604b","e53d5f48aeeb4fe393e427f24045a53d","365fe57c2416466cb3fe16d7c3d6d71f","02e3f153ae8545c8852fc95e546ec75c"]},"id":"QOUWCqyPfVd7","executionInfo":{"status":"ok","timestamp":1653107520565,"user_tz":-420,"elapsed":3436,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}},"outputId":"2b2e7d82-fcbd-4eca-8c4f-3e9e721549c4"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-e892ee5e9612a018/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-a604fc9c7d6eeda2.arrow\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d95761434164461a71a00c4c6e7a632"}},"metadata":{}}]},{"cell_type":"code","source":["tokenized_datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5R8U-jhkpq_","executionInfo":{"status":"ok","timestamp":1653107520566,"user_tz":-420,"elapsed":32,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}},"outputId":"6f42fa07-b3b8-4612-a478-d48a02df5061"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['labels', 'input_ids', 'attention_mask'],\n","        num_rows: 48483\n","    })\n","    val: Dataset({\n","        features: ['labels', 'input_ids', 'attention_mask'],\n","        num_rows: 3787\n","    })\n","})"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["tokenized_datasets['train'][0]['labels']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCTdOVe7X9Gc","executionInfo":{"status":"ok","timestamp":1653107520566,"user_tz":-420,"elapsed":26,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}},"outputId":"2e9e1cc4-a475-49c0-951d-8119cf73ba15"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["# Fine-tuning Roberta with Trainer API"],"metadata":{"id":"HU-3fh7Yozru"}},{"cell_type":"code","source":["from datasets import load_metric\n","\n","def compute_metrics(eval_preds):\n","    accuracy_metric = load_metric(\"accuracy\")\n","    precision_metric = load_metric(\"precision\")\n","    recall_metric = load_metric(\"recall\")\n","    f1_metric = load_metric(\"f1\")\n","\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","\n","    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n","    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n","    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n","    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n","\n","    return {\n","        \"accuracy\": accuracy['accuracy'],\n","        \"precision\": precision['precision'],\n","        \"recall\": recall['recall'],\n","        \"f1\": f1['f1'],\n","    }"],"metadata":{"id":"KybYJfNTTh9l","executionInfo":{"status":"ok","timestamp":1653107520567,"user_tz":-420,"elapsed":20,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    \"/content/output\",\n","    num_train_epochs=20,\n","    evaluation_strategy='epoch',\n","    load_best_model_at_end=True,\n","    metric_for_best_model = 'f1',\n","    learning_rate=2e-5,\n","    weight_decay=0.01,\n","    per_device_train_batch_size=64,\n","    per_device_eval_batch_size=64,\n","    save_strategy = \"epoch\",\n","    logging_steps=200\n",")"],"metadata":{"id":"lCa72JXPsslk","executionInfo":{"status":"ok","timestamp":1653107520567,"user_tz":-420,"elapsed":19,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# trainer=Trainer(\n","#     model=model,\n","#     args=training_args,\n","#     train_dataset=tokenized_datasets['train'],\n","#     eval_dataset=tokenized_datasets['val'],\n","#     compute_metrics=compute_metrics,\n","# )"],"metadata":{"id":"RCiLJRt1t7_h","executionInfo":{"status":"ok","timestamp":1653107520568,"user_tz":-420,"elapsed":19,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# trainer.train()"],"metadata":{"id":"wGO-h-hYx4xT","executionInfo":{"status":"ok","timestamp":1653107520569,"user_tz":-420,"elapsed":17,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameter search"],"metadata":{"id":"znKanSypYh1i"}},{"cell_type":"code","source":["! pip install -q optuna\n","! pip install -q ray[tune]"],"metadata":{"id":"WY0oS7prYknW","executionInfo":{"status":"ok","timestamp":1653107527058,"user_tz":-420,"elapsed":6504,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def model_init():\n","    return RobertaForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2\", num_labels=3)"],"metadata":{"id":"AthGnbNBYoQY","executionInfo":{"status":"ok","timestamp":1653107527059,"user_tz":-420,"elapsed":29,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["from transformers import EarlyStoppingCallback\n","\n","trainer=Trainer(\n","    model_init=model_init,\n","    args=training_args,\n","    train_dataset=tokenized_datasets['train'],\n","    eval_dataset=tokenized_datasets['val'],\n","    compute_metrics=compute_metrics,\n","    callbacks = [EarlyStoppingCallback(early_stopping_patience = 2)]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wy7in7G6Ys5L","executionInfo":{"status":"ok","timestamp":1653107531954,"user_tz":-420,"elapsed":4922,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}},"outputId":"7e3d20c9-bd36-44eb-f087-01b68ddbae1f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def my_hp_space(trial):\n","    return {\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-6, 5e-5, log=True),\n","        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 5, 10),\n","    }\n","\n","best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\", hp_space=my_hp_space)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3ba4472fe3944968996b500b43efef4f","db69fed13c724c18a4e0d6c766f5b4b2","0794002f1e044a64bb48356d19755235","fd506d5b6c06427184b853b51e182c10","a49b27a6997e4425ac320af32cede0ec","dbc02112494c43c4aed20774d9b2cda3","a46a42a963db4e8ebf057ceb874b3ea6","d1dea5bd4cba4a8996feb41a97ff3b64","a40c059849274940b994e61a37e93244","20c932beb38646dfa000db0b17323124","2ef7634a1269469cb5050689acb41268","ff8c5e67be204be28fa1036803fd2fcc","3af3cca8d4c84bee96cf8b52c1a3ab5e","2525f2db224744dcbd030b9b9c088a01","926692cf9336489bb85ad19aab28ecff","e0041d112b474b7a9cf6e7952492103a","3757d93263af40e490c21dec4e8408b8","a32875c532084f45949e99bd96062049","72fc8df569cb4eceab1b3c6aeb8afddb","d46374cb498840148f2f03fc0121bdb5","7c7c5a5f607a482e90bf8eb97c4ac151","19da2892c5374ab69da9e7a808b29dba","4ea189072b5e48b2a6f28d805d53c2fd","f704f7775fc34c09b444f4a8c3946554","f6c1ec0e27434ef28d1c68bdc6a9df02","39d0ac0298624841b0bfebd6027eb1e3","5002f764e5ca475bbbb488a2a9b0c36f","3f4078198e5b498ea140640b2a19895a","18461f3085c84bbf9a2c5a2d7d3eaaa5","2195edf05c774cce86251a49983391a4","9db6cb60c8084f9cb94915b0a6f7b6fc","c13531c8aa554a3b8802dce2d4416abc","425a37da9e4848d4aac2c6516ad5dc51","d5da14fa94f04127a2c1a4de34ebad48","0667efb759d444669e19e4ba5a473b88","03e2bd20b7bc46299a46076b929af2bd","18bfcaa443ce44f89b5f2127e27c4101","723454a38e454fdab6803aff71fca151","c33e569f9cab4a6b8d446f4047da94ee","616bb20034f9452388b84236f1b478a6","1664f87e0a6e43a596383c250f4e0bd1","dba851a91c074ea1a07a4fa40bc135d8","be5a4a36886641b697013917355697d0","1cd92de7f98f43cf8116f0082a6797d4"]},"id":"PQmujFMeYxTf","outputId":"d25f0ca8-537c-49fc-8e07-2483e81d382b","executionInfo":{"status":"ok","timestamp":1653130476822,"user_tz":-420,"elapsed":22944871,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-05-21 04:32:12,003]\u001b[0m A new study created in memory with name: no-name-b8c926f0-54b0-467b-a2b7-1710c05116cc\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5306\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5306' max='5306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5306/5306 1:05:11, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.401800</td>\n","      <td>0.387745</td>\n","      <td>0.864008</td>\n","      <td>0.865056</td>\n","      <td>0.839258</td>\n","      <td>0.850340</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.282600</td>\n","      <td>0.318619</td>\n","      <td>0.890151</td>\n","      <td>0.892633</td>\n","      <td>0.866506</td>\n","      <td>0.877790</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.212000</td>\n","      <td>0.325757</td>\n","      <td>0.902033</td>\n","      <td>0.903605</td>\n","      <td>0.881295</td>\n","      <td>0.891129</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.152600</td>\n","      <td>0.342127</td>\n","      <td>0.898336</td>\n","      <td>0.889423</td>\n","      <td>0.876773</td>\n","      <td>0.882559</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.105800</td>\n","      <td>0.398563</td>\n","      <td>0.896224</td>\n","      <td>0.885068</td>\n","      <td>0.878146</td>\n","      <td>0.881320</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.076900</td>\n","      <td>0.476940</td>\n","      <td>0.894904</td>\n","      <td>0.875697</td>\n","      <td>0.877180</td>\n","      <td>0.876417</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.047200</td>\n","      <td>0.524188</td>\n","      <td>0.894375</td>\n","      <td>0.873073</td>\n","      <td>0.876737</td>\n","      <td>0.874853</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ba4472fe3944968996b500b43efef4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/2.58k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff8c5e67be204be28fa1036803fd2fcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/2.52k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea189072b5e48b2a6f28d805d53c2fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5da14fa94f04127a2c1a4de34ebad48"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/output/run-0/checkpoint-758\n","Configuration saved in /content/output/run-0/checkpoint-758/config.json\n","Model weights saved in /content/output/run-0/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-0/checkpoint-1516\n","Configuration saved in /content/output/run-0/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-0/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-0/checkpoint-2274\n","Configuration saved in /content/output/run-0/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-0/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-0/checkpoint-3032\n","Configuration saved in /content/output/run-0/checkpoint-3032/config.json\n","Model weights saved in /content/output/run-0/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-0/checkpoint-3790\n","Configuration saved in /content/output/run-0/checkpoint-3790/config.json\n","Model weights saved in /content/output/run-0/checkpoint-3790/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-0/checkpoint-4548\n","Configuration saved in /content/output/run-0/checkpoint-4548/config.json\n","Model weights saved in /content/output/run-0/checkpoint-4548/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-0/checkpoint-5306\n","Configuration saved in /content/output/run-0/checkpoint-5306/config.json\n","Model weights saved in /content/output/run-0/checkpoint-5306/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-0/checkpoint-2274 (score: 0.8911289937447747).\n","\u001b[32m[I 2022-05-21 05:37:25,993]\u001b[0m Trial 0 finished with value: 3.5190383983523756 and parameters: {'learning_rate': 3.813488948910996e-05, 'num_train_epochs': 7}. Best is trial 0 with value: 3.5190383983523756.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7580\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='7580' max='7580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [7580/7580 1:33:12, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.481800</td>\n","      <td>0.439973</td>\n","      <td>0.835490</td>\n","      <td>0.830891</td>\n","      <td>0.811961</td>\n","      <td>0.819970</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.347300</td>\n","      <td>0.363453</td>\n","      <td>0.872458</td>\n","      <td>0.867334</td>\n","      <td>0.853394</td>\n","      <td>0.859788</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.283500</td>\n","      <td>0.356374</td>\n","      <td>0.881701</td>\n","      <td>0.885242</td>\n","      <td>0.852787</td>\n","      <td>0.866269</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.230900</td>\n","      <td>0.346377</td>\n","      <td>0.885661</td>\n","      <td>0.889837</td>\n","      <td>0.858359</td>\n","      <td>0.871522</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.193000</td>\n","      <td>0.386147</td>\n","      <td>0.884869</td>\n","      <td>0.882798</td>\n","      <td>0.863620</td>\n","      <td>0.872166</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.162500</td>\n","      <td>0.404146</td>\n","      <td>0.880908</td>\n","      <td>0.871826</td>\n","      <td>0.860460</td>\n","      <td>0.865711</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.129000</td>\n","      <td>0.443704</td>\n","      <td>0.878268</td>\n","      <td>0.874336</td>\n","      <td>0.850803</td>\n","      <td>0.861059</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.111500</td>\n","      <td>0.471183</td>\n","      <td>0.882229</td>\n","      <td>0.879641</td>\n","      <td>0.854853</td>\n","      <td>0.865517</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.099100</td>\n","      <td>0.486117</td>\n","      <td>0.883549</td>\n","      <td>0.875567</td>\n","      <td>0.862259</td>\n","      <td>0.868377</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.084900</td>\n","      <td>0.502710</td>\n","      <td>0.880116</td>\n","      <td>0.864502</td>\n","      <td>0.859024</td>\n","      <td>0.861668</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-758\n","Configuration saved in /content/output/run-1/checkpoint-758/config.json\n","Model weights saved in /content/output/run-1/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-1516\n","Configuration saved in /content/output/run-1/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-1/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-2274\n","Configuration saved in /content/output/run-1/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-1/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-3032\n","Configuration saved in /content/output/run-1/checkpoint-3032/config.json\n","Model weights saved in /content/output/run-1/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-3790\n","Configuration saved in /content/output/run-1/checkpoint-3790/config.json\n","Model weights saved in /content/output/run-1/checkpoint-3790/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-4548\n","Configuration saved in /content/output/run-1/checkpoint-4548/config.json\n","Model weights saved in /content/output/run-1/checkpoint-4548/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-5306\n","Configuration saved in /content/output/run-1/checkpoint-5306/config.json\n","Model weights saved in /content/output/run-1/checkpoint-5306/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-6064\n","Configuration saved in /content/output/run-1/checkpoint-6064/config.json\n","Model weights saved in /content/output/run-1/checkpoint-6064/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-6822\n","Configuration saved in /content/output/run-1/checkpoint-6822/config.json\n","Model weights saved in /content/output/run-1/checkpoint-6822/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-7580\n","Configuration saved in /content/output/run-1/checkpoint-7580/config.json\n","Model weights saved in /content/output/run-1/checkpoint-7580/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-1/checkpoint-3790 (score: 0.8721664862963553).\n","\u001b[32m[I 2022-05-21 07:10:40,639]\u001b[0m Trial 1 finished with value: 3.4653100899630545 and parameters: {'learning_rate': 1.276484425526061e-05, 'num_train_epochs': 10}. Best is trial 0 with value: 3.5190383983523756.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4548\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4548' max='4548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4548/4548 56:00, Epoch 6/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.404600</td>\n","      <td>0.375147</td>\n","      <td>0.865065</td>\n","      <td>0.860125</td>\n","      <td>0.847428</td>\n","      <td>0.853253</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.281500</td>\n","      <td>0.313335</td>\n","      <td>0.890679</td>\n","      <td>0.887483</td>\n","      <td>0.875372</td>\n","      <td>0.880882</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.203500</td>\n","      <td>0.314270</td>\n","      <td>0.903090</td>\n","      <td>0.908532</td>\n","      <td>0.879182</td>\n","      <td>0.891631</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.142100</td>\n","      <td>0.364125</td>\n","      <td>0.884605</td>\n","      <td>0.876207</td>\n","      <td>0.856841</td>\n","      <td>0.865341</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.099200</td>\n","      <td>0.402912</td>\n","      <td>0.894904</td>\n","      <td>0.889822</td>\n","      <td>0.874552</td>\n","      <td>0.881131</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.066700</td>\n","      <td>0.471677</td>\n","      <td>0.890415</td>\n","      <td>0.870831</td>\n","      <td>0.870161</td>\n","      <td>0.870482</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-2/checkpoint-758\n","Configuration saved in /content/output/run-2/checkpoint-758/config.json\n","Model weights saved in /content/output/run-2/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-2/checkpoint-1516\n","Configuration saved in /content/output/run-2/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-2/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-2/checkpoint-2274\n","Configuration saved in /content/output/run-2/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-2/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-2/checkpoint-3032\n","Configuration saved in /content/output/run-2/checkpoint-3032/config.json\n","Model weights saved in /content/output/run-2/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-2/checkpoint-3790\n","Configuration saved in /content/output/run-2/checkpoint-3790/config.json\n","Model weights saved in /content/output/run-2/checkpoint-3790/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-2/checkpoint-4548\n","Configuration saved in /content/output/run-2/checkpoint-4548/config.json\n","Model weights saved in /content/output/run-2/checkpoint-4548/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-2/checkpoint-2274 (score: 0.8916312815980182).\n","\u001b[32m[I 2022-05-21 08:06:43,318]\u001b[0m Trial 2 finished with value: 3.501888633965554 and parameters: {'learning_rate': 4.772710463287536e-05, 'num_train_epochs': 6}. Best is trial 0 with value: 3.5190383983523756.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6064\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6064' max='6064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6064/6064 1:14:40, Epoch 8/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.435000</td>\n","      <td>0.400664</td>\n","      <td>0.855558</td>\n","      <td>0.851422</td>\n","      <td>0.833737</td>\n","      <td>0.840914</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.305600</td>\n","      <td>0.328468</td>\n","      <td>0.887246</td>\n","      <td>0.887323</td>\n","      <td>0.868348</td>\n","      <td>0.876682</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.233900</td>\n","      <td>0.351096</td>\n","      <td>0.888038</td>\n","      <td>0.890389</td>\n","      <td>0.865658</td>\n","      <td>0.876291</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.178600</td>\n","      <td>0.343127</td>\n","      <td>0.895432</td>\n","      <td>0.890667</td>\n","      <td>0.875514</td>\n","      <td>0.882277</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.134000</td>\n","      <td>0.408541</td>\n","      <td>0.893583</td>\n","      <td>0.889857</td>\n","      <td>0.874206</td>\n","      <td>0.881355</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.104000</td>\n","      <td>0.462949</td>\n","      <td>0.885926</td>\n","      <td>0.873351</td>\n","      <td>0.865386</td>\n","      <td>0.869165</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.076600</td>\n","      <td>0.484849</td>\n","      <td>0.881965</td>\n","      <td>0.865561</td>\n","      <td>0.860242</td>\n","      <td>0.862786</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.064300</td>\n","      <td>0.540100</td>\n","      <td>0.881436</td>\n","      <td>0.864220</td>\n","      <td>0.860159</td>\n","      <td>0.862125</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-758\n","Configuration saved in /content/output/run-3/checkpoint-758/config.json\n","Model weights saved in /content/output/run-3/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-1516\n","Configuration saved in /content/output/run-3/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-3/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-2274\n","Configuration saved in /content/output/run-3/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-3/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-3032\n","Configuration saved in /content/output/run-3/checkpoint-3032/config.json\n","Model weights saved in /content/output/run-3/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-3790\n","Configuration saved in /content/output/run-3/checkpoint-3790/config.json\n","Model weights saved in /content/output/run-3/checkpoint-3790/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-4548\n","Configuration saved in /content/output/run-3/checkpoint-4548/config.json\n","Model weights saved in /content/output/run-3/checkpoint-4548/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-5306\n","Configuration saved in /content/output/run-3/checkpoint-5306/config.json\n","Model weights saved in /content/output/run-3/checkpoint-5306/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-6064\n","Configuration saved in /content/output/run-3/checkpoint-6064/config.json\n","Model weights saved in /content/output/run-3/checkpoint-6064/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-3/checkpoint-3032 (score: 0.8822766372667789).\n","\u001b[32m[I 2022-05-21 09:21:26,215]\u001b[0m Trial 3 finished with value: 3.467940838001349 and parameters: {'learning_rate': 2.2266830433189658e-05, 'num_train_epochs': 8}. Best is trial 0 with value: 3.5190383983523756.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3790\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3790' max='3790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3790/3790 46:40, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.743800</td>\n","      <td>0.700635</td>\n","      <td>0.700819</td>\n","      <td>0.681352</td>\n","      <td>0.664398</td>\n","      <td>0.670724</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.598100</td>\n","      <td>0.619657</td>\n","      <td>0.752311</td>\n","      <td>0.742979</td>\n","      <td>0.718010</td>\n","      <td>0.727297</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.548000</td>\n","      <td>0.568803</td>\n","      <td>0.776604</td>\n","      <td>0.767757</td>\n","      <td>0.742687</td>\n","      <td>0.752112</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.513600</td>\n","      <td>0.544778</td>\n","      <td>0.786374</td>\n","      <td>0.776381</td>\n","      <td>0.755557</td>\n","      <td>0.764278</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.490600</td>\n","      <td>0.545510</td>\n","      <td>0.790071</td>\n","      <td>0.778284</td>\n","      <td>0.762796</td>\n","      <td>0.769423</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-4/checkpoint-758\n","Configuration saved in /content/output/run-4/checkpoint-758/config.json\n","Model weights saved in /content/output/run-4/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-4/checkpoint-1516\n","Configuration saved in /content/output/run-4/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-4/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-4/checkpoint-2274\n","Configuration saved in /content/output/run-4/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-4/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-4/checkpoint-3032\n","Configuration saved in /content/output/run-4/checkpoint-3032/config.json\n","Model weights saved in /content/output/run-4/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-4/checkpoint-3790\n","Configuration saved in /content/output/run-4/checkpoint-3790/config.json\n","Model weights saved in /content/output/run-4/checkpoint-3790/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-4/checkpoint-3790 (score: 0.7694234189380603).\n","\u001b[32m[I 2022-05-21 10:08:09,342]\u001b[0m Trial 4 finished with value: 3.1005746363241102 and parameters: {'learning_rate': 2.1861028838225534e-06, 'num_train_epochs': 5}. Best is trial 0 with value: 3.5190383983523756.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4548\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='758' max='4548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 758/4548 09:14 < 46:22, 1.36 it/s, Epoch 1/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.560300</td>\n","      <td>0.509263</td>\n","      <td>0.799049</td>\n","      <td>0.799207</td>\n","      <td>0.760417</td>\n","      <td>0.775241</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","\u001b[32m[I 2022-05-21 10:17:26,654]\u001b[0m Trial 5 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4548\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='758' max='4548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 758/4548 09:15 < 46:22, 1.36 it/s, Epoch 1/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.521500</td>\n","      <td>0.475939</td>\n","      <td>0.813837</td>\n","      <td>0.810272</td>\n","      <td>0.783603</td>\n","      <td>0.794328</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","\u001b[32m[I 2022-05-21 10:26:44,053]\u001b[0m Trial 6 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7580\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='758' max='7580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 758/7580 09:14 < 1:23:27, 1.36 it/s, Epoch 1/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.688800</td>\n","      <td>0.639104</td>\n","      <td>0.738843</td>\n","      <td>0.728084</td>\n","      <td>0.702342</td>\n","      <td>0.711920</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","\u001b[32m[I 2022-05-21 10:36:01,271]\u001b[0m Trial 7 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3790\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='758' max='3790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 758/3790 09:15 < 37:06, 1.36 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.504400</td>\n","      <td>0.459060</td>\n","      <td>0.825456</td>\n","      <td>0.822690</td>\n","      <td>0.797883</td>\n","      <td>0.807979</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","\u001b[32m[I 2022-05-21 10:45:18,762]\u001b[0m Trial 8 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6064\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='758' max='6064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 758/6064 09:14 < 1:04:54, 1.36 it/s, Epoch 1/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.539200</td>\n","      <td>0.490431</td>\n","      <td>0.810140</td>\n","      <td>0.805588</td>\n","      <td>0.775810</td>\n","      <td>0.787784</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","\u001b[32m[I 2022-05-21 10:54:35,977]\u001b[0m Trial 9 pruned. \u001b[0m\n"]}]},{"cell_type":"code","source":["best_run"],"metadata":{"id":"K-6BlFBVizbn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653134161911,"user_tz":-420,"elapsed":3,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}},"outputId":"c8b93ccd-5997-4987-fa25-8b19383af3bb"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BestRun(run_id='0', objective=3.5190383983523756, hyperparameters={'learning_rate': 3.813488948910996e-05, 'num_train_epochs': 7})"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["for n, v in best_run.hyperparameters.items():\n","    setattr(trainer.args, n, v)\n","\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wnPBgwuTd6a1","executionInfo":{"status":"ok","timestamp":1653140837063,"user_tz":-420,"elapsed":3930407,"user":{"displayName":"Cong Phu Nguyen","userId":"03211082518667393153"}},"outputId":"4e18aa33-d215-4c01-9359-fc33002cccba"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_2 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5306\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5306' max='5306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5306/5306 1:05:27, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.401800</td>\n","      <td>0.387745</td>\n","      <td>0.864008</td>\n","      <td>0.865056</td>\n","      <td>0.839258</td>\n","      <td>0.850340</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.282600</td>\n","      <td>0.318619</td>\n","      <td>0.890151</td>\n","      <td>0.892633</td>\n","      <td>0.866506</td>\n","      <td>0.877790</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.212000</td>\n","      <td>0.325757</td>\n","      <td>0.902033</td>\n","      <td>0.903605</td>\n","      <td>0.881295</td>\n","      <td>0.891129</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.152600</td>\n","      <td>0.342127</td>\n","      <td>0.898336</td>\n","      <td>0.889423</td>\n","      <td>0.876773</td>\n","      <td>0.882559</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.105800</td>\n","      <td>0.398563</td>\n","      <td>0.896224</td>\n","      <td>0.885068</td>\n","      <td>0.878146</td>\n","      <td>0.881320</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.076900</td>\n","      <td>0.476940</td>\n","      <td>0.894904</td>\n","      <td>0.875697</td>\n","      <td>0.877180</td>\n","      <td>0.876417</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.047200</td>\n","      <td>0.524188</td>\n","      <td>0.894375</td>\n","      <td>0.873073</td>\n","      <td>0.876737</td>\n","      <td>0.874853</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/checkpoint-758\n","Configuration saved in /content/output/checkpoint-758/config.json\n","Model weights saved in /content/output/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/checkpoint-1516\n","Configuration saved in /content/output/checkpoint-1516/config.json\n","Model weights saved in /content/output/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/checkpoint-2274\n","Configuration saved in /content/output/checkpoint-2274/config.json\n","Model weights saved in /content/output/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/checkpoint-3032\n","Configuration saved in /content/output/checkpoint-3032/config.json\n","Model weights saved in /content/output/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/checkpoint-3790\n","Configuration saved in /content/output/checkpoint-3790/config.json\n","Model weights saved in /content/output/checkpoint-3790/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/checkpoint-4548\n","Configuration saved in /content/output/checkpoint-4548/config.json\n","Model weights saved in /content/output/checkpoint-4548/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/checkpoint-5306\n","Configuration saved in /content/output/checkpoint-5306/config.json\n","Model weights saved in /content/output/checkpoint-5306/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/checkpoint-2274 (score: 0.8911289937447747).\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=5306, training_loss=0.2007515748581525, metrics={'train_runtime': 3928.0897, 'train_samples_per_second': 86.398, 'train_steps_per_second': 1.351, 'total_flos': 2.24123298138432e+16, 'train_loss': 0.2007515748581525, 'epoch': 7.0})"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":[""],"metadata":{"id":"XMDVi58oejVU"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Roberta_MLM_SA_AUG.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9ed850e334534d119516ff2fd6670436":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4739ab6b5c04efc8dd8b8694b5db9a8","IPY_MODEL_a09ae2fea20442f0a52bea2762b9279c","IPY_MODEL_adc9f1f21fd54845b9f9ae2afdf52446"],"layout":"IPY_MODEL_bf96e5420b5b4f62a5d116cabe640b14"}},"c4739ab6b5c04efc8dd8b8694b5db9a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7864736a87241e8a60d5a60b8306d9f","placeholder":"​","style":"IPY_MODEL_82e2b9586ac7407bb49cc62dd33e085f","value":"100%"}},"a09ae2fea20442f0a52bea2762b9279c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b96a32ebb13430cafda10eb8d66aac0","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d25cc6ca2486479f8d4645579309c50e","value":2}},"adc9f1f21fd54845b9f9ae2afdf52446":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc5f53b1fd284d5ab606b7f53caaa7f5","placeholder":"​","style":"IPY_MODEL_cc1fc4d14eac4b52a5ffd1c1ad19c39d","value":" 2/2 [00:00&lt;00:00, 62.22it/s]"}},"bf96e5420b5b4f62a5d116cabe640b14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7864736a87241e8a60d5a60b8306d9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82e2b9586ac7407bb49cc62dd33e085f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b96a32ebb13430cafda10eb8d66aac0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d25cc6ca2486479f8d4645579309c50e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc5f53b1fd284d5ab606b7f53caaa7f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc1fc4d14eac4b52a5ffd1c1ad19c39d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d95761434164461a71a00c4c6e7a632":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c533aa653a474d698ec9732e2e30c2f3","IPY_MODEL_70cb991b03ce47578b17656d88c9f1d6","IPY_MODEL_046c29f93b5b463f9ea7f0380e2ae8fe"],"layout":"IPY_MODEL_c27bd27b1bd54160bb8142098b5a542a"}},"c533aa653a474d698ec9732e2e30c2f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6b1df7a8a55447791983b0b0d7ed3f9","placeholder":"​","style":"IPY_MODEL_331108bd35cf4954a671fda40bcae4cd","value":"100%"}},"70cb991b03ce47578b17656d88c9f1d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c098c882ba374c0cb3bf1f3901e3604b","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e53d5f48aeeb4fe393e427f24045a53d","value":4}},"046c29f93b5b463f9ea7f0380e2ae8fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_365fe57c2416466cb3fe16d7c3d6d71f","placeholder":"​","style":"IPY_MODEL_02e3f153ae8545c8852fc95e546ec75c","value":" 4/4 [00:02&lt;00:00,  2.02ba/s]"}},"c27bd27b1bd54160bb8142098b5a542a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6b1df7a8a55447791983b0b0d7ed3f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"331108bd35cf4954a671fda40bcae4cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c098c882ba374c0cb3bf1f3901e3604b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e53d5f48aeeb4fe393e427f24045a53d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"365fe57c2416466cb3fe16d7c3d6d71f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02e3f153ae8545c8852fc95e546ec75c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ba4472fe3944968996b500b43efef4f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db69fed13c724c18a4e0d6c766f5b4b2","IPY_MODEL_0794002f1e044a64bb48356d19755235","IPY_MODEL_fd506d5b6c06427184b853b51e182c10"],"layout":"IPY_MODEL_a49b27a6997e4425ac320af32cede0ec"}},"db69fed13c724c18a4e0d6c766f5b4b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbc02112494c43c4aed20774d9b2cda3","placeholder":"​","style":"IPY_MODEL_a46a42a963db4e8ebf057ceb874b3ea6","value":"Downloading builder script: "}},"0794002f1e044a64bb48356d19755235":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1dea5bd4cba4a8996feb41a97ff3b64","max":1652,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a40c059849274940b994e61a37e93244","value":1652}},"fd506d5b6c06427184b853b51e182c10":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20c932beb38646dfa000db0b17323124","placeholder":"​","style":"IPY_MODEL_2ef7634a1269469cb5050689acb41268","value":" 4.21k/? [00:00&lt;00:00, 192kB/s]"}},"a49b27a6997e4425ac320af32cede0ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbc02112494c43c4aed20774d9b2cda3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a46a42a963db4e8ebf057ceb874b3ea6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1dea5bd4cba4a8996feb41a97ff3b64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a40c059849274940b994e61a37e93244":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20c932beb38646dfa000db0b17323124":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ef7634a1269469cb5050689acb41268":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff8c5e67be204be28fa1036803fd2fcc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3af3cca8d4c84bee96cf8b52c1a3ab5e","IPY_MODEL_2525f2db224744dcbd030b9b9c088a01","IPY_MODEL_926692cf9336489bb85ad19aab28ecff"],"layout":"IPY_MODEL_e0041d112b474b7a9cf6e7952492103a"}},"3af3cca8d4c84bee96cf8b52c1a3ab5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3757d93263af40e490c21dec4e8408b8","placeholder":"​","style":"IPY_MODEL_a32875c532084f45949e99bd96062049","value":"Downloading builder script: "}},"2525f2db224744dcbd030b9b9c088a01":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72fc8df569cb4eceab1b3c6aeb8afddb","max":2575,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d46374cb498840148f2f03fc0121bdb5","value":2575}},"926692cf9336489bb85ad19aab28ecff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c7c5a5f607a482e90bf8eb97c4ac151","placeholder":"​","style":"IPY_MODEL_19da2892c5374ab69da9e7a808b29dba","value":" 7.55k/? [00:00&lt;00:00, 303kB/s]"}},"e0041d112b474b7a9cf6e7952492103a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3757d93263af40e490c21dec4e8408b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a32875c532084f45949e99bd96062049":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72fc8df569cb4eceab1b3c6aeb8afddb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d46374cb498840148f2f03fc0121bdb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c7c5a5f607a482e90bf8eb97c4ac151":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19da2892c5374ab69da9e7a808b29dba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ea189072b5e48b2a6f28d805d53c2fd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f704f7775fc34c09b444f4a8c3946554","IPY_MODEL_f6c1ec0e27434ef28d1c68bdc6a9df02","IPY_MODEL_39d0ac0298624841b0bfebd6027eb1e3"],"layout":"IPY_MODEL_5002f764e5ca475bbbb488a2a9b0c36f"}},"f704f7775fc34c09b444f4a8c3946554":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f4078198e5b498ea140640b2a19895a","placeholder":"​","style":"IPY_MODEL_18461f3085c84bbf9a2c5a2d7d3eaaa5","value":"Downloading builder script: "}},"f6c1ec0e27434ef28d1c68bdc6a9df02":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2195edf05c774cce86251a49983391a4","max":2524,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9db6cb60c8084f9cb94915b0a6f7b6fc","value":2524}},"39d0ac0298624841b0bfebd6027eb1e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c13531c8aa554a3b8802dce2d4416abc","placeholder":"​","style":"IPY_MODEL_425a37da9e4848d4aac2c6516ad5dc51","value":" 7.38k/? [00:00&lt;00:00, 285kB/s]"}},"5002f764e5ca475bbbb488a2a9b0c36f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f4078198e5b498ea140640b2a19895a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18461f3085c84bbf9a2c5a2d7d3eaaa5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2195edf05c774cce86251a49983391a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9db6cb60c8084f9cb94915b0a6f7b6fc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c13531c8aa554a3b8802dce2d4416abc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"425a37da9e4848d4aac2c6516ad5dc51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5da14fa94f04127a2c1a4de34ebad48":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0667efb759d444669e19e4ba5a473b88","IPY_MODEL_03e2bd20b7bc46299a46076b929af2bd","IPY_MODEL_18bfcaa443ce44f89b5f2127e27c4101"],"layout":"IPY_MODEL_723454a38e454fdab6803aff71fca151"}},"0667efb759d444669e19e4ba5a473b88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c33e569f9cab4a6b8d446f4047da94ee","placeholder":"​","style":"IPY_MODEL_616bb20034f9452388b84236f1b478a6","value":"Downloading builder script: "}},"03e2bd20b7bc46299a46076b929af2bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1664f87e0a6e43a596383c250f4e0bd1","max":2318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dba851a91c074ea1a07a4fa40bc135d8","value":2318}},"18bfcaa443ce44f89b5f2127e27c4101":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be5a4a36886641b697013917355697d0","placeholder":"​","style":"IPY_MODEL_1cd92de7f98f43cf8116f0082a6797d4","value":" 6.50k/? [00:00&lt;00:00, 256kB/s]"}},"723454a38e454fdab6803aff71fca151":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c33e569f9cab4a6b8d446f4047da94ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"616bb20034f9452388b84236f1b478a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1664f87e0a6e43a596383c250f4e0bd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dba851a91c074ea1a07a4fa40bc135d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be5a4a36886641b697013917355697d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cd92de7f98f43cf8116f0082a6797d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}