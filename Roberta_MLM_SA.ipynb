{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":560,"status":"ok","timestamp":1653096665509,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"},"user_tz":-420},"id":"UJGL0qzsah8u","outputId":"a4fb2400-2cae-43f8-b063-ab02b2004967"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat May 21 01:31:04 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":18305,"status":"ok","timestamp":1653096684180,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"},"user_tz":-420},"id":"EtEk6V7iOfhd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e996e757-2e01-4ab7-aa62-bc33a40a4ad0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.2 MB 7.1 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 34.1 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 56.2 MB/s \n","\u001b[K     |████████████████████████████████| 84 kB 3.1 MB/s \n","\u001b[K     |████████████████████████████████| 346 kB 9.8 MB/s \n","\u001b[K     |████████████████████████████████| 212 kB 48.3 MB/s \n","\u001b[K     |████████████████████████████████| 140 kB 54.2 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 49.4 MB/s \n","\u001b[K     |████████████████████████████████| 127 kB 63.7 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 61.3 MB/s \n","\u001b[K     |████████████████████████████████| 94 kB 2.5 MB/s \n","\u001b[K     |████████████████████████████████| 144 kB 55.1 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q transformers\n","!pip install -q datasets"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7980,"status":"ok","timestamp":1653096692154,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"},"user_tz":-420},"id":"zar7AVFcOzfS"},"outputs":[],"source":["#transformers\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import RandomOverSampler \n","import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import Trainer, TrainingArguments"]},{"cell_type":"code","source":["tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1\", num_labels=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["e133453bd8e54a95a85d1acbcae464b5","00d1fcda55b2413dbb0bb3832386c9aa","85abb45287b7499cb79ba80c0cfae15e","5ee2e3ba3f3c4950a5f8868aa5fe3096","9969ea4a7f974f73ac18c1b0145fbdda","859085a799164799b092b971276dfc73","1677d60f02974634b2159f26d604d89f","8c3639d035d54b8189dbc233349bf599","43e9e50022724b8fb48674c57205feae","d7bcd303edcd43b6b23e1be57608adba","dc7dfcb454f645d2b1b93afed3cc4c5f","abca3e0f3e2e4869bef50d9d7f84e0d9","363f355841f84b70adb3d132038a0bdc","254b6f0bccd04100a53653641e54b7ac","1259fafb9d15450ea5632f0c1f14c4d5","a52529bc642a485daba6fd327a16dc82","2d26ee47e26643bdbbe9d276987c5c4b","c9010c918ebd4eaf9cc7a029a5d5f7ca","6c1c45d9b2294900a8818e19d5d74050","778b0fbcc1f4421989a3d3913998de89","ac42001b89b64fc887cb21c7dc8f11a2","32e7fdb5c15e49b6a6253cdd16ba0b9e","7257b0910f5a4bdfa46f2ba02507cac2","dd51ccb8a97e467b84cc690a8fae6f77","201f7ccc8e8645c190a82a53ae615373","5b4fbe2bb26b455a8612c6a62ed66747","fdb63c2e060a47328a2e5499cf812919","b0541ccb0a864489b2c5d573a6b56a62","00172524d2a444cd8b86a94920c4606b","4b1ed84c068747c2ac1491053a5b804b","6cfcbc5517c44c3b8d412c3789065705","8ad5bd4ba4c04a74b1213345d8ffa296","f92ec4fa29e5489ea27f4cb12ebc920c"]},"id":"SoFp7KcAh1aX","executionInfo":{"status":"ok","timestamp":1653096703096,"user_tz":-420,"elapsed":10957,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}},"outputId":"9d909f1f-b8fc-439d-d591-3302933373cf"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e133453bd8e54a95a85d1acbcae464b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abca3e0f3e2e4869bef50d9d7f84e0d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7257b0910f5a4bdfa46f2ba02507cac2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["model"],"metadata":{"id":"fmZJ4iBeWMV8","executionInfo":{"status":"ok","timestamp":1653096703096,"user_tz":-420,"elapsed":28,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}},"outputId":"495f6b3f-b699-48c1-a603-e92a90a4a00b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"ciWQXgp0SAD3"},"source":["# Load Dataset\n"]},{"cell_type":"code","source":["train_dataset_path = \"/content/drive/MyDrive/NLP/Setiment_Analysis/en/dataset/covid-19/aug_clean_train.csv\"\n","test_dataset_path = \"/content/drive/MyDrive/NLP/Setiment_Analysis/en/dataset/covid-19/clean_test.csv\""],"metadata":{"id":"RhWshjVggda9","executionInfo":{"status":"ok","timestamp":1653096703097,"user_tz":-420,"elapsed":22,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset, load_dataset\n","\n","dataset = load_dataset('csv', data_files={'train': train_dataset_path,\n","                                          'val': test_dataset_path})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":185,"referenced_widgets":["5f6bfce8b1bd4fa7a4d1547b5f6040e8","4647f9bd0e4a49cf8b964641a2d21d8d","b8b5067b90814865be62716353d403d1","14f58a53e66b4c85a0bbdbe882d3fa24","da28d5319a514b3a91d23e4602928d44","e8eddedceaa74ec88744bf455d670439","7b2e88fc0efb472ebde6f89d49890d47","381cb00b0b5a418f905a718c370c87eb","5dc4e27d8fe64d8ba54e85ff829bc85a","c00495013c064877ab7e922d09cedad0","cc3a35ec106f44d5b7bfe241b5372608","650fb0cbcade44649c98a546cc6d0f50","f424552b30414a7f9371dc2335b84e12","22928dce3acf49b9a9d19cc57ac10a21","048f0a2df03049f2879e0da57ec4e303","c6746a058c2446d98111a90bc4f951d0","bd29902def4b41399b9ab238471f2682","c29cedc411b54dc3833851d5f947e272","12fa34612a6f4c65b4626fa97e982a66","36edca10cc05491685dfe78871e70d0a","f37cbd53529940dc8217af8c03cac0e4","86bec46a11a444ca80cdbfd5697a03b4","0af8c1456151494ba10b6e15866459a9","fb7843dce0ee423e8984ff6a536d4dc5","278b90b257f24b0989b095c398373ae1","2d3f3348260d440fbcebbb487aa716fa","343ddd8b2ed64a2f9acaa0d5118ae4f4","3b7fa66a2cc141329e1096c09d953e5e","3aaeaab0d3694fa3908d3156914d9d08","1afea42d077548848e690e5a44f3f88b","cc497c0d154742979df2a674a75c23e9","4e6fa0f39cab482db20a1c6ee61be0e4","fb4682b3898748b7981feeef19c89a06"]},"id":"pWE5R3ylesdm","executionInfo":{"status":"ok","timestamp":1653096704914,"user_tz":-420,"elapsed":1838,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}},"outputId":"cd62d2fa-54aa-4ea7-f1ee-16be264e2030"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Using custom data configuration default-c23d9ec77aab9cc0\n"]},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-c23d9ec77aab9cc0/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f6bfce8b1bd4fa7a4d1547b5f6040e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"650fb0cbcade44649c98a546cc6d0f50"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-c23d9ec77aab9cc0/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0af8c1456151494ba10b6e15866459a9"}},"metadata":{}}]},{"cell_type":"markdown","source":["## Preprocess data"],"metadata":{"id":"9sgbjZx9o6G2"}},{"cell_type":"code","source":["dataset = dataset.rename_column(\"text_clean\", \"texts\")\n","dataset = dataset.rename_column(\"Sentiment\", \"labels\")\n","# dataset['train'], dataset['validation'] = dataset['train'].train_test_split(0.2).values()"],"metadata":{"id":"t5htSYiRjY8l","executionInfo":{"status":"ok","timestamp":1653096704915,"user_tz":-420,"elapsed":11,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def label_mapping(examples):\n","    label_to_idx = {'Extremely Negative': 0, 'Negative': 0, 'Neutral': 1,\n","                    'Positive': 2, 'Extremely Positive': 2}\n","    examples[\"labels\"] = label_to_idx[examples[\"labels\"]]\n","    return examples\n","    \n","dataset = dataset.map(label_mapping)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["9082e85e9e2b4623b1cfc7305e6c8cb3","ca7e28ce2d264d9580869246ac1e31b3","3a72b481c0a64ff78926a909f302cfb3","e42d7ea3015d47e2bf7fa0c47620d4ca","157f8f02320647a0a27da9ac363ab497","aa5560499b56440db41edef876e1c092","79baa155460245e99db355a1fc0dface","407fa14533f04228ae720aa36891fa19","e0dae47c4b10475380f0353dc8426a2e","621ac34bd4344984b69ef60fe8fc20ea","7a45f03c81a94ef6ba50d47da4c57b75","fdcca65a6bad4e53bb68260226423fc9","f89925532296401aa2a647044c488086","69850b3edaac4f2cac14f4ebcbda862c","f4c4d02836fa440196c0ecec9301c6e0","c6b8767204e74ee6a8128598d180c877","e4ad3d657f8a4945b39b68c54339bb40","ce3601b878e24b37bd8253b1f82cea00","827daae5d06045d494e7f91eb3dde967","3b6ba7cdf1454a67b556dbf092d7d76b","4999c6e13c624115ad27ad6d64076a84","c20e04de80b0479a872bdfe21ff271aa"]},"id":"MT_CQVthj75v","executionInfo":{"status":"ok","timestamp":1653096710192,"user_tz":-420,"elapsed":5287,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}},"outputId":"28610ddb-5017-4c11-e9e5-3fa8dd6e33c2"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/48483 [00:00<?, ?ex/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9082e85e9e2b4623b1cfc7305e6c8cb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3787 [00:00<?, ?ex/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdcca65a6bad4e53bb68260226423fc9"}},"metadata":{}}]},{"cell_type":"code","source":["def tokenizer_function(examples):\n","    result = tokenizer(examples[\"texts\"], max_length=128,\n","                       padding='max_length', truncation=True)\n","    result[\"labels\"] = examples[\"labels\"]\n","    return result\n","\n","tokenized_datasets = dataset.map(tokenizer_function, batched=True, remove_columns=['texts', 'labels'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["608b3e8783484c738c97eebec5534eca","efe6e0755b0341fda40781fe065f7f65","707709aa96f34bc7afe1aef31932a404","6f84944bee754045b5879a37c2de6c4b","5784e01c33f54659bf04d97ea4110958","1829314d9ebd48fa9bd46b9151e0d157","d3a426575c0a4b7eac6262e482eb1c40","6d69673fe8a249c892aff008085acecf","876cb2c56efd4b9384c6ef3dbfc6c341","18752f9b918545f88f8fdb86121f4760","54d3a51d7cd54037b3833607a54354e9","36b61a1b235343d3a270d9ea64362471","cfc6e556aa53440f8ff595cba59ece24","b261f3af5b6648ed92a31fbc552e9956","cfbfe4149aa6440a9ac793b202a0789f","95b79bf8281141d3893a9d091ed13c82","d189f0bef6164cfea72bb55d4d0ecf8e","baa7a29c40f14b0d9f14015e1615dfa6","d6523e57776a4f01ace260ac9a748d70","9db9128c24714aeabbda989541144785","bfcfa34efcee486fb41aedfa3adad429","0daf46c156fe488cb99dd3f712ac617a"]},"id":"QOUWCqyPfVd7","executionInfo":{"status":"ok","timestamp":1653096736338,"user_tz":-420,"elapsed":26177,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}},"outputId":"c08ace51-b0b4-4ee9-c437-13ae1f942695"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/49 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"608b3e8783484c738c97eebec5534eca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36b61a1b235343d3a270d9ea64362471"}},"metadata":{}}]},{"cell_type":"code","source":["tokenized_datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5R8U-jhkpq_","executionInfo":{"status":"ok","timestamp":1653096736701,"user_tz":-420,"elapsed":11,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}},"outputId":"84726d06-75da-43df-a853-40d2ead26589"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['labels', 'input_ids', 'attention_mask'],\n","        num_rows: 48483\n","    })\n","    val: Dataset({\n","        features: ['labels', 'input_ids', 'attention_mask'],\n","        num_rows: 3787\n","    })\n","})"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["tokenized_datasets['train'][0]['labels']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCTdOVe7X9Gc","executionInfo":{"status":"ok","timestamp":1653096736701,"user_tz":-420,"elapsed":10,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}},"outputId":"ae1abb0f-65b9-44b3-9ba5-5499d135c188"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["# Fine-tuning Roberta with Trainer API"],"metadata":{"id":"HU-3fh7Yozru"}},{"cell_type":"code","source":["from datasets import load_metric\n","\n","def compute_metrics(eval_preds):\n","    accuracy_metric = load_metric(\"accuracy\")\n","    precision_metric = load_metric(\"precision\")\n","    recall_metric = load_metric(\"recall\")\n","    f1_metric = load_metric(\"f1\")\n","\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","\n","    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n","    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n","    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n","    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n","\n","    return {\n","        \"accuracy\": accuracy['accuracy'],\n","        \"precision\": precision['precision'],\n","        \"recall\": recall['recall'],\n","        \"f1\": f1['f1'],\n","    }"],"metadata":{"id":"KybYJfNTTh9l","executionInfo":{"status":"ok","timestamp":1653096736702,"user_tz":-420,"elapsed":9,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    \"/content/output\",\n","    num_train_epochs=20,\n","    evaluation_strategy='epoch',\n","    load_best_model_at_end=True,\n","    metric_for_best_model = 'f1',\n","    learning_rate=2e-5,\n","    weight_decay=0.01,\n","    per_device_train_batch_size=64,\n","    per_device_eval_batch_size=64,\n","    save_strategy = \"epoch\",\n","    logging_steps=200\n",")"],"metadata":{"id":"lCa72JXPsslk","executionInfo":{"status":"ok","timestamp":1653096736702,"user_tz":-420,"elapsed":9,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# trainer=Trainer(\n","#     model=model,\n","#     args=training_args,\n","#     train_dataset=tokenized_datasets['train'],\n","#     eval_dataset=tokenized_datasets['val'],\n","#     compute_metrics=compute_metrics,\n","# )"],"metadata":{"id":"RCiLJRt1t7_h","executionInfo":{"status":"ok","timestamp":1653096736702,"user_tz":-420,"elapsed":8,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# trainer.train()"],"metadata":{"id":"wGO-h-hYx4xT","executionInfo":{"status":"ok","timestamp":1653096736703,"user_tz":-420,"elapsed":9,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameter search"],"metadata":{"id":"znKanSypYh1i"}},{"cell_type":"code","source":["! pip install -q optuna\n","! pip install -q ray[tune]"],"metadata":{"id":"WY0oS7prYknW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653096758960,"user_tz":-420,"elapsed":22266,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}},"outputId":"1f5d37f9-d190-4006-bd66-85a09a9798e9"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 308 kB 7.7 MB/s \n","\u001b[K     |████████████████████████████████| 210 kB 61.6 MB/s \n","\u001b[K     |████████████████████████████████| 81 kB 7.9 MB/s \n","\u001b[K     |████████████████████████████████| 78 kB 6.4 MB/s \n","\u001b[K     |████████████████████████████████| 49 kB 5.8 MB/s \n","\u001b[K     |████████████████████████████████| 112 kB 63.7 MB/s \n","\u001b[K     |████████████████████████████████| 146 kB 64.5 MB/s \n","\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 53.2 MB 1.2 MB/s \n","\u001b[K     |████████████████████████████████| 4.1 MB 42.9 MB/s \n","\u001b[K     |████████████████████████████████| 8.8 MB 50.9 MB/s \n","\u001b[K     |████████████████████████████████| 125 kB 54.4 MB/s \n","\u001b[K     |████████████████████████████████| 461 kB 56.0 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["def model_init():\n","    return RobertaForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1\", num_labels=3)"],"metadata":{"id":"AthGnbNBYoQY","executionInfo":{"status":"ok","timestamp":1653096758960,"user_tz":-420,"elapsed":35,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["trainer=Trainer(\n","    model_init=model_init,\n","    args=training_args,\n","    train_dataset=tokenized_datasets['train'],\n","    eval_dataset=tokenized_datasets['val'],\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wy7in7G6Ys5L","executionInfo":{"status":"ok","timestamp":1653096776317,"user_tz":-420,"elapsed":17390,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}},"outputId":"64d9935c-c687-495b-e779-1f33da619001"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def my_hp_space(trial):\n","    return {\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 5e-5, log=True),\n","        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 7),\n","    }\n","\n","best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\", hp_space=my_hp_space)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1098dc52f3d04ed3bfe1c1dda190a920","11ccf0aa13404e4294c2ce1c930da7f8","d06f71bee8d34bc7918fccf39d10cb13","032b7e5ee0db403c81fa361269881fa7","5ebf340ebf134df18f5ccea6823de61a","8f544b48d6884240b9dbf997e55fa01b","3dd5bf1eb024493e8e8ca6d4b4608c6d","6fae40fd338e41a1b8807571ec559e71","bc4ffd17ab1f474dab89c1bdbcc2cc0e","8448799776a644999cd90735a60e34c0","852347fbc0414442959a629465680401","34353cb58b37474da80baa3b0ba1f910","b7a23aee804b49e6a1393bcee837b5d5","9ea27ea2bd2b4b9a86837ed70393b649","e63189b72f4940268e2d7e1b96ae426c","83f0deeb71a843288ba08857705bd7a2","70a86d482a6646619f3a4c7216e63af3","f14793a27b834b059c6ad6b92c01b811","54d7ec07dfb84552b41aad7293cf2e21","cc862120d1ee4a0383db0f7913b3946e","9017665d4c2640f9b978c26537a4e892","cf9f8d97ecde4468999216e90f88c086","58ab411aa72e4e0caad2a68259a7fdd7","71e37e15efad4fa698dfed8713d5ad03","2f31eb4aee6e434dbee98fd13de7e567","6683d6ca39b842bd94d066b9844798df","3fef1eccee54437ebcabc3d9922f18cc","c9ab82f3187e47ec93ff380010257266","42ec3df33ee74bbba61d7928368cbbb4","16d9cf636d554c209546f6556c444db8","0a2d2884069f43bb94b9576aca442efb","8bbabc5dacb34b3d9aeae0314cc50b5b","85772050819340138e74f75e1e6c553c","c8fae328cb4740788f879b90fdb07b0d","32926dba443e4ecba1e6c919e51ab282","67c2581280ac4d3eba33740a59dc0a20","c8baad3154dd425f8232d25057f36590","1185ce4988da4557b553e8f9c00e8c07","56deec79b6fb4a4cb12e78784040bc07","69f5595882b44d4e8ad6d18d7ee36ef8","e412b5e5e69a46b397ba218e455b6ad7","285d0a71fe39437cbd72108aed52c83a","e26343c18d0342119f49f49e0bd8ec41","2f43a513bcc44691b84a17a2b42f1ab2"]},"id":"PQmujFMeYxTf","outputId":"4a93ec2b-9292-42ee-bf0a-671ea6161214","executionInfo":{"status":"ok","timestamp":1653124003995,"user_tz":-420,"elapsed":27227714,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-05-21 01:32:56,387]\u001b[0m A new study created in memory with name: no-name-9609b58f-59d8-480c-b083-b7e07ab39a11\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5306\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5306' max='5306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5306/5306 1:05:59, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.832700</td>\n","      <td>0.750909</td>\n","      <td>0.676789</td>\n","      <td>0.646650</td>\n","      <td>0.652148</td>\n","      <td>0.647383</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.669100</td>\n","      <td>0.662723</td>\n","      <td>0.731186</td>\n","      <td>0.711971</td>\n","      <td>0.700017</td>\n","      <td>0.703782</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.610300</td>\n","      <td>0.614052</td>\n","      <td>0.753895</td>\n","      <td>0.742549</td>\n","      <td>0.720388</td>\n","      <td>0.727998</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.564900</td>\n","      <td>0.574222</td>\n","      <td>0.771059</td>\n","      <td>0.757911</td>\n","      <td>0.742347</td>\n","      <td>0.748955</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.532100</td>\n","      <td>0.565821</td>\n","      <td>0.779773</td>\n","      <td>0.767976</td>\n","      <td>0.749528</td>\n","      <td>0.757521</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.516600</td>\n","      <td>0.558118</td>\n","      <td>0.780565</td>\n","      <td>0.773071</td>\n","      <td>0.746545</td>\n","      <td>0.757245</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.500500</td>\n","      <td>0.556641</td>\n","      <td>0.780565</td>\n","      <td>0.769145</td>\n","      <td>0.750627</td>\n","      <td>0.758512</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1098dc52f3d04ed3bfe1c1dda190a920"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/2.58k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34353cb58b37474da80baa3b0ba1f910"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/2.52k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ab411aa72e4e0caad2a68259a7fdd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8fae328cb4740788f879b90fdb07b0d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/output/run-0/checkpoint-758\n","Configuration saved in /content/output/run-0/checkpoint-758/config.json\n","Model weights saved in /content/output/run-0/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-0/checkpoint-1516\n","Configuration saved in /content/output/run-0/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-0/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-0/checkpoint-2274\n","Configuration saved in /content/output/run-0/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-0/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-0/checkpoint-3032\n","Configuration saved in /content/output/run-0/checkpoint-3032/config.json\n","Model weights saved in /content/output/run-0/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-0/checkpoint-3790\n","Configuration saved in /content/output/run-0/checkpoint-3790/config.json\n","Model weights saved in /content/output/run-0/checkpoint-3790/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-0/checkpoint-4548\n","Configuration saved in /content/output/run-0/checkpoint-4548/config.json\n","Model weights saved in /content/output/run-0/checkpoint-4548/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-0/checkpoint-5306\n","Configuration saved in /content/output/run-0/checkpoint-5306/config.json\n","Model weights saved in /content/output/run-0/checkpoint-5306/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-0/checkpoint-5306 (score: 0.7585124608279671).\n","\u001b[32m[I 2022-05-21 02:38:58,607]\u001b[0m Trial 0 finished with value: 3.05884884157638 and parameters: {'learning_rate': 1.475760256526725e-06, 'num_train_epochs': 7}. Best is trial 0 with value: 3.05884884157638.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2274\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2274' max='2274' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2274/2274 28:20, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.526700</td>\n","      <td>0.474627</td>\n","      <td>0.818590</td>\n","      <td>0.815323</td>\n","      <td>0.789676</td>\n","      <td>0.799963</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.377300</td>\n","      <td>0.421692</td>\n","      <td>0.847901</td>\n","      <td>0.848334</td>\n","      <td>0.820767</td>\n","      <td>0.832008</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.328600</td>\n","      <td>0.397975</td>\n","      <td>0.862424</td>\n","      <td>0.857621</td>\n","      <td>0.836158</td>\n","      <td>0.845591</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-758\n","Configuration saved in /content/output/run-1/checkpoint-758/config.json\n","Model weights saved in /content/output/run-1/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-1516\n","Configuration saved in /content/output/run-1/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-1/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-1/checkpoint-2274\n","Configuration saved in /content/output/run-1/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-1/checkpoint-2274/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-1/checkpoint-2274 (score: 0.8455905259649948).\n","\u001b[32m[I 2022-05-21 03:07:21,697]\u001b[0m Trial 1 finished with value: 3.401793498164615 and parameters: {'learning_rate': 1.1205971711566048e-05, 'num_train_epochs': 3}. Best is trial 1 with value: 3.401793498164615.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3790\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3790' max='3790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3790/3790 47:15, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.787900</td>\n","      <td>0.711247</td>\n","      <td>0.696065</td>\n","      <td>0.667873</td>\n","      <td>0.666025</td>\n","      <td>0.666069</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.632900</td>\n","      <td>0.632764</td>\n","      <td>0.747029</td>\n","      <td>0.730361</td>\n","      <td>0.716127</td>\n","      <td>0.721287</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.578700</td>\n","      <td>0.588939</td>\n","      <td>0.765778</td>\n","      <td>0.754032</td>\n","      <td>0.731625</td>\n","      <td>0.739705</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.544700</td>\n","      <td>0.562304</td>\n","      <td>0.777396</td>\n","      <td>0.762260</td>\n","      <td>0.750438</td>\n","      <td>0.755594</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.519600</td>\n","      <td>0.561881</td>\n","      <td>0.779773</td>\n","      <td>0.766926</td>\n","      <td>0.751298</td>\n","      <td>0.758001</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-2/checkpoint-758\n","Configuration saved in /content/output/run-2/checkpoint-758/config.json\n","Model weights saved in /content/output/run-2/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-2/checkpoint-1516\n","Configuration saved in /content/output/run-2/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-2/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-2/checkpoint-2274\n","Configuration saved in /content/output/run-2/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-2/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-2/checkpoint-3032\n","Configuration saved in /content/output/run-2/checkpoint-3032/config.json\n","Model weights saved in /content/output/run-2/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-2/checkpoint-3790\n","Configuration saved in /content/output/run-2/checkpoint-3790/config.json\n","Model weights saved in /content/output/run-2/checkpoint-3790/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-2/checkpoint-3790 (score: 0.7580009844185202).\n","\u001b[32m[I 2022-05-21 03:54:40,327]\u001b[0m Trial 2 finished with value: 3.0559978031971413 and parameters: {'learning_rate': 1.9540030993653547e-06, 'num_train_epochs': 5}. Best is trial 1 with value: 3.401793498164615.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4548\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4548' max='4548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4548/4548 56:43, Epoch 6/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.874500</td>\n","      <td>0.791561</td>\n","      <td>0.656984</td>\n","      <td>0.628402</td>\n","      <td>0.640004</td>\n","      <td>0.630628</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.712800</td>\n","      <td>0.694968</td>\n","      <td>0.707684</td>\n","      <td>0.685402</td>\n","      <td>0.675288</td>\n","      <td>0.678324</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.657100</td>\n","      <td>0.655748</td>\n","      <td>0.731978</td>\n","      <td>0.711872</td>\n","      <td>0.702333</td>\n","      <td>0.704925</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.615700</td>\n","      <td>0.624219</td>\n","      <td>0.745973</td>\n","      <td>0.725374</td>\n","      <td>0.718353</td>\n","      <td>0.721227</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.588200</td>\n","      <td>0.612615</td>\n","      <td>0.752311</td>\n","      <td>0.737012</td>\n","      <td>0.719385</td>\n","      <td>0.726659</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.576700</td>\n","      <td>0.609974</td>\n","      <td>0.754423</td>\n","      <td>0.738227</td>\n","      <td>0.721721</td>\n","      <td>0.728543</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-758\n","Configuration saved in /content/output/run-3/checkpoint-758/config.json\n","Model weights saved in /content/output/run-3/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-1516\n","Configuration saved in /content/output/run-3/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-3/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-2274\n","Configuration saved in /content/output/run-3/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-3/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-3032\n","Configuration saved in /content/output/run-3/checkpoint-3032/config.json\n","Model weights saved in /content/output/run-3/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-3790\n","Configuration saved in /content/output/run-3/checkpoint-3790/config.json\n","Model weights saved in /content/output/run-3/checkpoint-3790/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-3/checkpoint-4548\n","Configuration saved in /content/output/run-3/checkpoint-4548/config.json\n","Model weights saved in /content/output/run-3/checkpoint-4548/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-3/checkpoint-4548 (score: 0.7285427596685875).\n","\u001b[32m[I 2022-05-21 04:51:26,866]\u001b[0m Trial 3 finished with value: 2.942913692352239 and parameters: {'learning_rate': 1.1715663983495218e-06, 'num_train_epochs': 6}. Best is trial 1 with value: 3.401793498164615.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4548\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4548' max='4548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4548/4548 56:42, Epoch 6/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.836300</td>\n","      <td>0.755816</td>\n","      <td>0.673092</td>\n","      <td>0.642334</td>\n","      <td>0.649238</td>\n","      <td>0.643655</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.675000</td>\n","      <td>0.667731</td>\n","      <td>0.728017</td>\n","      <td>0.708866</td>\n","      <td>0.695847</td>\n","      <td>0.700145</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.619500</td>\n","      <td>0.623123</td>\n","      <td>0.751254</td>\n","      <td>0.736607</td>\n","      <td>0.717360</td>\n","      <td>0.724054</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.577500</td>\n","      <td>0.587748</td>\n","      <td>0.766306</td>\n","      <td>0.750793</td>\n","      <td>0.736905</td>\n","      <td>0.742832</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.548000</td>\n","      <td>0.579230</td>\n","      <td>0.771851</td>\n","      <td>0.759874</td>\n","      <td>0.739375</td>\n","      <td>0.747950</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.537200</td>\n","      <td>0.576801</td>\n","      <td>0.771587</td>\n","      <td>0.759001</td>\n","      <td>0.741065</td>\n","      <td>0.748573</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-4/checkpoint-758\n","Configuration saved in /content/output/run-4/checkpoint-758/config.json\n","Model weights saved in /content/output/run-4/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-4/checkpoint-1516\n","Configuration saved in /content/output/run-4/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-4/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-4/checkpoint-2274\n","Configuration saved in /content/output/run-4/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-4/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-4/checkpoint-3032\n","Configuration saved in /content/output/run-4/checkpoint-3032/config.json\n","Model weights saved in /content/output/run-4/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-4/checkpoint-3790\n","Configuration saved in /content/output/run-4/checkpoint-3790/config.json\n","Model weights saved in /content/output/run-4/checkpoint-3790/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-4/checkpoint-4548\n","Configuration saved in /content/output/run-4/checkpoint-4548/config.json\n","Model weights saved in /content/output/run-4/checkpoint-4548/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-4/checkpoint-4548 (score: 0.748573018754796).\n","\u001b[32m[I 2022-05-21 05:48:12,318]\u001b[0m Trial 4 finished with value: 3.02022581224263 and parameters: {'learning_rate': 1.4543773233810393e-06, 'num_train_epochs': 6}. Best is trial 1 with value: 3.401793498164615.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3790\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3790' max='3790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3790/3790 47:16, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.657300</td>\n","      <td>0.581997</td>\n","      <td>0.770003</td>\n","      <td>0.762364</td>\n","      <td>0.732722</td>\n","      <td>0.744251</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.505800</td>\n","      <td>0.522866</td>\n","      <td>0.801954</td>\n","      <td>0.790861</td>\n","      <td>0.772128</td>\n","      <td>0.779805</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.447600</td>\n","      <td>0.480292</td>\n","      <td>0.818854</td>\n","      <td>0.813041</td>\n","      <td>0.788280</td>\n","      <td>0.798302</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.414800</td>\n","      <td>0.460576</td>\n","      <td>0.828360</td>\n","      <td>0.821134</td>\n","      <td>0.799220</td>\n","      <td>0.808623</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.386900</td>\n","      <td>0.460736</td>\n","      <td>0.831793</td>\n","      <td>0.823675</td>\n","      <td>0.800989</td>\n","      <td>0.810691</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-5/checkpoint-758\n","Configuration saved in /content/output/run-5/checkpoint-758/config.json\n","Model weights saved in /content/output/run-5/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-5/checkpoint-1516\n","Configuration saved in /content/output/run-5/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-5/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-5/checkpoint-2274\n","Configuration saved in /content/output/run-5/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-5/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-5/checkpoint-3032\n","Configuration saved in /content/output/run-5/checkpoint-3032/config.json\n","Model weights saved in /content/output/run-5/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-5/checkpoint-3790\n","Configuration saved in /content/output/run-5/checkpoint-3790/config.json\n","Model weights saved in /content/output/run-5/checkpoint-3790/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-5/checkpoint-3790 (score: 0.8106906232864066).\n","\u001b[32m[I 2022-05-21 06:35:31,916]\u001b[0m Trial 5 finished with value: 3.2671474177759956 and parameters: {'learning_rate': 4.307344104644919e-06, 'num_train_epochs': 5}. Best is trial 1 with value: 3.401793498164615.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4548\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4548' max='4548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4548/4548 56:39, Epoch 6/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.781600</td>\n","      <td>0.704855</td>\n","      <td>0.699498</td>\n","      <td>0.672018</td>\n","      <td>0.669024</td>\n","      <td>0.669485</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.624000</td>\n","      <td>0.623102</td>\n","      <td>0.752311</td>\n","      <td>0.736850</td>\n","      <td>0.722012</td>\n","      <td>0.727431</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.564800</td>\n","      <td>0.576936</td>\n","      <td>0.772115</td>\n","      <td>0.762583</td>\n","      <td>0.738622</td>\n","      <td>0.747161</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.526500</td>\n","      <td>0.543513</td>\n","      <td>0.785582</td>\n","      <td>0.773399</td>\n","      <td>0.756352</td>\n","      <td>0.763716</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.495300</td>\n","      <td>0.539393</td>\n","      <td>0.787431</td>\n","      <td>0.776510</td>\n","      <td>0.756319</td>\n","      <td>0.764996</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.483000</td>\n","      <td>0.536713</td>\n","      <td>0.791392</td>\n","      <td>0.779354</td>\n","      <td>0.760955</td>\n","      <td>0.768835</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-6/checkpoint-758\n","Configuration saved in /content/output/run-6/checkpoint-758/config.json\n","Model weights saved in /content/output/run-6/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-6/checkpoint-1516\n","Configuration saved in /content/output/run-6/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-6/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-6/checkpoint-2274\n","Configuration saved in /content/output/run-6/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-6/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-6/checkpoint-3032\n","Configuration saved in /content/output/run-6/checkpoint-3032/config.json\n","Model weights saved in /content/output/run-6/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-6/checkpoint-3790\n","Configuration saved in /content/output/run-6/checkpoint-3790/config.json\n","Model weights saved in /content/output/run-6/checkpoint-3790/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-6/checkpoint-4548\n","Configuration saved in /content/output/run-6/checkpoint-4548/config.json\n","Model weights saved in /content/output/run-6/checkpoint-4548/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-6/checkpoint-4548 (score: 0.7688351637819723).\n","\u001b[32m[I 2022-05-21 07:32:14,361]\u001b[0m Trial 6 finished with value: 3.1005365052333778 and parameters: {'learning_rate': 2.000108810215921e-06, 'num_train_epochs': 6}. Best is trial 1 with value: 3.401793498164615.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2274\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2274' max='2274' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2274/2274 28:19, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.656100</td>\n","      <td>0.585285</td>\n","      <td>0.769210</td>\n","      <td>0.759837</td>\n","      <td>0.733642</td>\n","      <td>0.743866</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.515600</td>\n","      <td>0.536139</td>\n","      <td>0.796145</td>\n","      <td>0.786574</td>\n","      <td>0.765992</td>\n","      <td>0.774044</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.475900</td>\n","      <td>0.515634</td>\n","      <td>0.799313</td>\n","      <td>0.785017</td>\n","      <td>0.772574</td>\n","      <td>0.778057</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-7/checkpoint-758\n","Configuration saved in /content/output/run-7/checkpoint-758/config.json\n","Model weights saved in /content/output/run-7/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-7/checkpoint-1516\n","Configuration saved in /content/output/run-7/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-7/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-7/checkpoint-2274\n","Configuration saved in /content/output/run-7/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-7/checkpoint-2274/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-7/checkpoint-2274 (score: 0.7780566101666686).\n","\u001b[32m[I 2022-05-21 08:00:37,087]\u001b[0m Trial 7 finished with value: 3.134960616407666 and parameters: {'learning_rate': 4.616752684037717e-06, 'num_train_epochs': 3}. Best is trial 1 with value: 3.401793498164615.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3032\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1516' max='3032' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1516/3032 18:47 < 18:49, 1.34 it/s, Epoch 2/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.779900</td>\n","      <td>0.706958</td>\n","      <td>0.698178</td>\n","      <td>0.670449</td>\n","      <td>0.668034</td>\n","      <td>0.668372</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.630700</td>\n","      <td>0.632496</td>\n","      <td>0.749142</td>\n","      <td>0.733399</td>\n","      <td>0.716817</td>\n","      <td>0.722967</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-8/checkpoint-758\n","Configuration saved in /content/output/run-8/checkpoint-758/config.json\n","Model weights saved in /content/output/run-8/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","\u001b[32m[I 2022-05-21 08:19:28,031]\u001b[0m Trial 8 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.19.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/NLP/Setiment_Analysis/en/save_pretrained/exp_1 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 48483\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3790\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3790' max='3790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3790/3790 47:12, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.401100</td>\n","      <td>0.390509</td>\n","      <td>0.863480</td>\n","      <td>0.863000</td>\n","      <td>0.841036</td>\n","      <td>0.850573</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.280800</td>\n","      <td>0.318483</td>\n","      <td>0.890943</td>\n","      <td>0.888099</td>\n","      <td>0.873840</td>\n","      <td>0.880289</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.202300</td>\n","      <td>0.344758</td>\n","      <td>0.896752</td>\n","      <td>0.902217</td>\n","      <td>0.871351</td>\n","      <td>0.884461</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.137300</td>\n","      <td>0.352822</td>\n","      <td>0.895696</td>\n","      <td>0.887245</td>\n","      <td>0.869709</td>\n","      <td>0.877616</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.095400</td>\n","      <td>0.414347</td>\n","      <td>0.893583</td>\n","      <td>0.880282</td>\n","      <td>0.873507</td>\n","      <td>0.876647</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-9/checkpoint-758\n","Configuration saved in /content/output/run-9/checkpoint-758/config.json\n","Model weights saved in /content/output/run-9/checkpoint-758/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-9/checkpoint-1516\n","Configuration saved in /content/output/run-9/checkpoint-1516/config.json\n","Model weights saved in /content/output/run-9/checkpoint-1516/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-9/checkpoint-2274\n","Configuration saved in /content/output/run-9/checkpoint-2274/config.json\n","Model weights saved in /content/output/run-9/checkpoint-2274/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-9/checkpoint-3032\n","Configuration saved in /content/output/run-9/checkpoint-3032/config.json\n","Model weights saved in /content/output/run-9/checkpoint-3032/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 3787\n","  Batch size = 64\n","Saving model checkpoint to /content/output/run-9/checkpoint-3790\n","Configuration saved in /content/output/run-9/checkpoint-3790/config.json\n","Model weights saved in /content/output/run-9/checkpoint-3790/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/output/run-9/checkpoint-2274 (score: 0.8844611971876551).\n","\u001b[32m[I 2022-05-21 09:06:43,382]\u001b[0m Trial 9 finished with value: 3.524019399778715 and parameters: {'learning_rate': 4.5085613945129333e-05, 'num_train_epochs': 5}. Best is trial 9 with value: 3.524019399778715.\u001b[0m\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"K-6BlFBVizbn","executionInfo":{"status":"ok","timestamp":1653124003996,"user_tz":-420,"elapsed":41,"user":{"displayName":"Cong Phu Nguyen","userId":"16550869234069995488"}}},"execution_count":21,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Roberta_MLM_SA.ipynb","provenance":[],"mount_file_id":"1nzqAnE0AiLMRpKhQ_pwjVT7oMMu4vCk6","authorship_tag":"ABX9TyO8ssd1Zx+uCJye5jz42YwX"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e133453bd8e54a95a85d1acbcae464b5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00d1fcda55b2413dbb0bb3832386c9aa","IPY_MODEL_85abb45287b7499cb79ba80c0cfae15e","IPY_MODEL_5ee2e3ba3f3c4950a5f8868aa5fe3096"],"layout":"IPY_MODEL_9969ea4a7f974f73ac18c1b0145fbdda"}},"00d1fcda55b2413dbb0bb3832386c9aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_859085a799164799b092b971276dfc73","placeholder":"​","style":"IPY_MODEL_1677d60f02974634b2159f26d604d89f","value":"Downloading: 100%"}},"85abb45287b7499cb79ba80c0cfae15e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c3639d035d54b8189dbc233349bf599","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_43e9e50022724b8fb48674c57205feae","value":898823}},"5ee2e3ba3f3c4950a5f8868aa5fe3096":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7bcd303edcd43b6b23e1be57608adba","placeholder":"​","style":"IPY_MODEL_dc7dfcb454f645d2b1b93afed3cc4c5f","value":" 878k/878k [00:00&lt;00:00, 2.10MB/s]"}},"9969ea4a7f974f73ac18c1b0145fbdda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"859085a799164799b092b971276dfc73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1677d60f02974634b2159f26d604d89f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c3639d035d54b8189dbc233349bf599":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43e9e50022724b8fb48674c57205feae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7bcd303edcd43b6b23e1be57608adba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc7dfcb454f645d2b1b93afed3cc4c5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"abca3e0f3e2e4869bef50d9d7f84e0d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_363f355841f84b70adb3d132038a0bdc","IPY_MODEL_254b6f0bccd04100a53653641e54b7ac","IPY_MODEL_1259fafb9d15450ea5632f0c1f14c4d5"],"layout":"IPY_MODEL_a52529bc642a485daba6fd327a16dc82"}},"363f355841f84b70adb3d132038a0bdc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d26ee47e26643bdbbe9d276987c5c4b","placeholder":"​","style":"IPY_MODEL_c9010c918ebd4eaf9cc7a029a5d5f7ca","value":"Downloading: 100%"}},"254b6f0bccd04100a53653641e54b7ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c1c45d9b2294900a8818e19d5d74050","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_778b0fbcc1f4421989a3d3913998de89","value":456318}},"1259fafb9d15450ea5632f0c1f14c4d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac42001b89b64fc887cb21c7dc8f11a2","placeholder":"​","style":"IPY_MODEL_32e7fdb5c15e49b6a6253cdd16ba0b9e","value":" 446k/446k [00:00&lt;00:00, 683kB/s]"}},"a52529bc642a485daba6fd327a16dc82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d26ee47e26643bdbbe9d276987c5c4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9010c918ebd4eaf9cc7a029a5d5f7ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c1c45d9b2294900a8818e19d5d74050":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"778b0fbcc1f4421989a3d3913998de89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac42001b89b64fc887cb21c7dc8f11a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32e7fdb5c15e49b6a6253cdd16ba0b9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7257b0910f5a4bdfa46f2ba02507cac2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dd51ccb8a97e467b84cc690a8fae6f77","IPY_MODEL_201f7ccc8e8645c190a82a53ae615373","IPY_MODEL_5b4fbe2bb26b455a8612c6a62ed66747"],"layout":"IPY_MODEL_fdb63c2e060a47328a2e5499cf812919"}},"dd51ccb8a97e467b84cc690a8fae6f77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0541ccb0a864489b2c5d573a6b56a62","placeholder":"​","style":"IPY_MODEL_00172524d2a444cd8b86a94920c4606b","value":"Downloading: 100%"}},"201f7ccc8e8645c190a82a53ae615373":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b1ed84c068747c2ac1491053a5b804b","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6cfcbc5517c44c3b8d412c3789065705","value":481}},"5b4fbe2bb26b455a8612c6a62ed66747":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ad5bd4ba4c04a74b1213345d8ffa296","placeholder":"​","style":"IPY_MODEL_f92ec4fa29e5489ea27f4cb12ebc920c","value":" 481/481 [00:00&lt;00:00, 16.5kB/s]"}},"fdb63c2e060a47328a2e5499cf812919":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0541ccb0a864489b2c5d573a6b56a62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00172524d2a444cd8b86a94920c4606b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b1ed84c068747c2ac1491053a5b804b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cfcbc5517c44c3b8d412c3789065705":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ad5bd4ba4c04a74b1213345d8ffa296":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f92ec4fa29e5489ea27f4cb12ebc920c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f6bfce8b1bd4fa7a4d1547b5f6040e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4647f9bd0e4a49cf8b964641a2d21d8d","IPY_MODEL_b8b5067b90814865be62716353d403d1","IPY_MODEL_14f58a53e66b4c85a0bbdbe882d3fa24"],"layout":"IPY_MODEL_da28d5319a514b3a91d23e4602928d44"}},"4647f9bd0e4a49cf8b964641a2d21d8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8eddedceaa74ec88744bf455d670439","placeholder":"​","style":"IPY_MODEL_7b2e88fc0efb472ebde6f89d49890d47","value":"Downloading data files: 100%"}},"b8b5067b90814865be62716353d403d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_381cb00b0b5a418f905a718c370c87eb","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5dc4e27d8fe64d8ba54e85ff829bc85a","value":2}},"14f58a53e66b4c85a0bbdbe882d3fa24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c00495013c064877ab7e922d09cedad0","placeholder":"​","style":"IPY_MODEL_cc3a35ec106f44d5b7bfe241b5372608","value":" 2/2 [00:00&lt;00:00,  9.69it/s]"}},"da28d5319a514b3a91d23e4602928d44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8eddedceaa74ec88744bf455d670439":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b2e88fc0efb472ebde6f89d49890d47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"381cb00b0b5a418f905a718c370c87eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dc4e27d8fe64d8ba54e85ff829bc85a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c00495013c064877ab7e922d09cedad0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc3a35ec106f44d5b7bfe241b5372608":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"650fb0cbcade44649c98a546cc6d0f50":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f424552b30414a7f9371dc2335b84e12","IPY_MODEL_22928dce3acf49b9a9d19cc57ac10a21","IPY_MODEL_048f0a2df03049f2879e0da57ec4e303"],"layout":"IPY_MODEL_c6746a058c2446d98111a90bc4f951d0"}},"f424552b30414a7f9371dc2335b84e12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd29902def4b41399b9ab238471f2682","placeholder":"​","style":"IPY_MODEL_c29cedc411b54dc3833851d5f947e272","value":"Extracting data files: 100%"}},"22928dce3acf49b9a9d19cc57ac10a21":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_12fa34612a6f4c65b4626fa97e982a66","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36edca10cc05491685dfe78871e70d0a","value":2}},"048f0a2df03049f2879e0da57ec4e303":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f37cbd53529940dc8217af8c03cac0e4","placeholder":"​","style":"IPY_MODEL_86bec46a11a444ca80cdbfd5697a03b4","value":" 2/2 [00:00&lt;00:00, 28.36it/s]"}},"c6746a058c2446d98111a90bc4f951d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd29902def4b41399b9ab238471f2682":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c29cedc411b54dc3833851d5f947e272":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12fa34612a6f4c65b4626fa97e982a66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36edca10cc05491685dfe78871e70d0a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f37cbd53529940dc8217af8c03cac0e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86bec46a11a444ca80cdbfd5697a03b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0af8c1456151494ba10b6e15866459a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fb7843dce0ee423e8984ff6a536d4dc5","IPY_MODEL_278b90b257f24b0989b095c398373ae1","IPY_MODEL_2d3f3348260d440fbcebbb487aa716fa"],"layout":"IPY_MODEL_343ddd8b2ed64a2f9acaa0d5118ae4f4"}},"fb7843dce0ee423e8984ff6a536d4dc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b7fa66a2cc141329e1096c09d953e5e","placeholder":"​","style":"IPY_MODEL_3aaeaab0d3694fa3908d3156914d9d08","value":"100%"}},"278b90b257f24b0989b095c398373ae1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1afea42d077548848e690e5a44f3f88b","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc497c0d154742979df2a674a75c23e9","value":2}},"2d3f3348260d440fbcebbb487aa716fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e6fa0f39cab482db20a1c6ee61be0e4","placeholder":"​","style":"IPY_MODEL_fb4682b3898748b7981feeef19c89a06","value":" 2/2 [00:00&lt;00:00, 56.22it/s]"}},"343ddd8b2ed64a2f9acaa0d5118ae4f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b7fa66a2cc141329e1096c09d953e5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3aaeaab0d3694fa3908d3156914d9d08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1afea42d077548848e690e5a44f3f88b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc497c0d154742979df2a674a75c23e9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e6fa0f39cab482db20a1c6ee61be0e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb4682b3898748b7981feeef19c89a06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9082e85e9e2b4623b1cfc7305e6c8cb3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ca7e28ce2d264d9580869246ac1e31b3","IPY_MODEL_3a72b481c0a64ff78926a909f302cfb3","IPY_MODEL_e42d7ea3015d47e2bf7fa0c47620d4ca"],"layout":"IPY_MODEL_157f8f02320647a0a27da9ac363ab497"}},"ca7e28ce2d264d9580869246ac1e31b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa5560499b56440db41edef876e1c092","placeholder":"​","style":"IPY_MODEL_79baa155460245e99db355a1fc0dface","value":"100%"}},"3a72b481c0a64ff78926a909f302cfb3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_407fa14533f04228ae720aa36891fa19","max":48483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0dae47c4b10475380f0353dc8426a2e","value":48483}},"e42d7ea3015d47e2bf7fa0c47620d4ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_621ac34bd4344984b69ef60fe8fc20ea","placeholder":"​","style":"IPY_MODEL_7a45f03c81a94ef6ba50d47da4c57b75","value":" 48483/48483 [00:04&lt;00:00, 9932.26ex/s]"}},"157f8f02320647a0a27da9ac363ab497":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa5560499b56440db41edef876e1c092":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79baa155460245e99db355a1fc0dface":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"407fa14533f04228ae720aa36891fa19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0dae47c4b10475380f0353dc8426a2e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"621ac34bd4344984b69ef60fe8fc20ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a45f03c81a94ef6ba50d47da4c57b75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdcca65a6bad4e53bb68260226423fc9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f89925532296401aa2a647044c488086","IPY_MODEL_69850b3edaac4f2cac14f4ebcbda862c","IPY_MODEL_f4c4d02836fa440196c0ecec9301c6e0"],"layout":"IPY_MODEL_c6b8767204e74ee6a8128598d180c877"}},"f89925532296401aa2a647044c488086":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4ad3d657f8a4945b39b68c54339bb40","placeholder":"​","style":"IPY_MODEL_ce3601b878e24b37bd8253b1f82cea00","value":"100%"}},"69850b3edaac4f2cac14f4ebcbda862c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_827daae5d06045d494e7f91eb3dde967","max":3787,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3b6ba7cdf1454a67b556dbf092d7d76b","value":3787}},"f4c4d02836fa440196c0ecec9301c6e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4999c6e13c624115ad27ad6d64076a84","placeholder":"​","style":"IPY_MODEL_c20e04de80b0479a872bdfe21ff271aa","value":" 3787/3787 [00:00&lt;00:00, 10667.83ex/s]"}},"c6b8767204e74ee6a8128598d180c877":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4ad3d657f8a4945b39b68c54339bb40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce3601b878e24b37bd8253b1f82cea00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"827daae5d06045d494e7f91eb3dde967":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b6ba7cdf1454a67b556dbf092d7d76b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4999c6e13c624115ad27ad6d64076a84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c20e04de80b0479a872bdfe21ff271aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"608b3e8783484c738c97eebec5534eca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_efe6e0755b0341fda40781fe065f7f65","IPY_MODEL_707709aa96f34bc7afe1aef31932a404","IPY_MODEL_6f84944bee754045b5879a37c2de6c4b"],"layout":"IPY_MODEL_5784e01c33f54659bf04d97ea4110958"}},"efe6e0755b0341fda40781fe065f7f65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1829314d9ebd48fa9bd46b9151e0d157","placeholder":"​","style":"IPY_MODEL_d3a426575c0a4b7eac6262e482eb1c40","value":"100%"}},"707709aa96f34bc7afe1aef31932a404":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d69673fe8a249c892aff008085acecf","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_876cb2c56efd4b9384c6ef3dbfc6c341","value":49}},"6f84944bee754045b5879a37c2de6c4b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18752f9b918545f88f8fdb86121f4760","placeholder":"​","style":"IPY_MODEL_54d3a51d7cd54037b3833607a54354e9","value":" 49/49 [00:22&lt;00:00,  2.70ba/s]"}},"5784e01c33f54659bf04d97ea4110958":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1829314d9ebd48fa9bd46b9151e0d157":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3a426575c0a4b7eac6262e482eb1c40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d69673fe8a249c892aff008085acecf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"876cb2c56efd4b9384c6ef3dbfc6c341":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"18752f9b918545f88f8fdb86121f4760":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54d3a51d7cd54037b3833607a54354e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36b61a1b235343d3a270d9ea64362471":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cfc6e556aa53440f8ff595cba59ece24","IPY_MODEL_b261f3af5b6648ed92a31fbc552e9956","IPY_MODEL_cfbfe4149aa6440a9ac793b202a0789f"],"layout":"IPY_MODEL_95b79bf8281141d3893a9d091ed13c82"}},"cfc6e556aa53440f8ff595cba59ece24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d189f0bef6164cfea72bb55d4d0ecf8e","placeholder":"​","style":"IPY_MODEL_baa7a29c40f14b0d9f14015e1615dfa6","value":"100%"}},"b261f3af5b6648ed92a31fbc552e9956":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6523e57776a4f01ace260ac9a748d70","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9db9128c24714aeabbda989541144785","value":4}},"cfbfe4149aa6440a9ac793b202a0789f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfcfa34efcee486fb41aedfa3adad429","placeholder":"​","style":"IPY_MODEL_0daf46c156fe488cb99dd3f712ac617a","value":" 4/4 [00:01&lt;00:00,  2.18ba/s]"}},"95b79bf8281141d3893a9d091ed13c82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d189f0bef6164cfea72bb55d4d0ecf8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"baa7a29c40f14b0d9f14015e1615dfa6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6523e57776a4f01ace260ac9a748d70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9db9128c24714aeabbda989541144785":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bfcfa34efcee486fb41aedfa3adad429":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0daf46c156fe488cb99dd3f712ac617a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1098dc52f3d04ed3bfe1c1dda190a920":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11ccf0aa13404e4294c2ce1c930da7f8","IPY_MODEL_d06f71bee8d34bc7918fccf39d10cb13","IPY_MODEL_032b7e5ee0db403c81fa361269881fa7"],"layout":"IPY_MODEL_5ebf340ebf134df18f5ccea6823de61a"}},"11ccf0aa13404e4294c2ce1c930da7f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f544b48d6884240b9dbf997e55fa01b","placeholder":"​","style":"IPY_MODEL_3dd5bf1eb024493e8e8ca6d4b4608c6d","value":"Downloading builder script: "}},"d06f71bee8d34bc7918fccf39d10cb13":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fae40fd338e41a1b8807571ec559e71","max":1652,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc4ffd17ab1f474dab89c1bdbcc2cc0e","value":1652}},"032b7e5ee0db403c81fa361269881fa7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8448799776a644999cd90735a60e34c0","placeholder":"​","style":"IPY_MODEL_852347fbc0414442959a629465680401","value":" 4.21k/? [00:00&lt;00:00, 109kB/s]"}},"5ebf340ebf134df18f5ccea6823de61a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f544b48d6884240b9dbf997e55fa01b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dd5bf1eb024493e8e8ca6d4b4608c6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fae40fd338e41a1b8807571ec559e71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc4ffd17ab1f474dab89c1bdbcc2cc0e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8448799776a644999cd90735a60e34c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"852347fbc0414442959a629465680401":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34353cb58b37474da80baa3b0ba1f910":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7a23aee804b49e6a1393bcee837b5d5","IPY_MODEL_9ea27ea2bd2b4b9a86837ed70393b649","IPY_MODEL_e63189b72f4940268e2d7e1b96ae426c"],"layout":"IPY_MODEL_83f0deeb71a843288ba08857705bd7a2"}},"b7a23aee804b49e6a1393bcee837b5d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70a86d482a6646619f3a4c7216e63af3","placeholder":"​","style":"IPY_MODEL_f14793a27b834b059c6ad6b92c01b811","value":"Downloading builder script: "}},"9ea27ea2bd2b4b9a86837ed70393b649":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_54d7ec07dfb84552b41aad7293cf2e21","max":2575,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc862120d1ee4a0383db0f7913b3946e","value":2575}},"e63189b72f4940268e2d7e1b96ae426c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9017665d4c2640f9b978c26537a4e892","placeholder":"​","style":"IPY_MODEL_cf9f8d97ecde4468999216e90f88c086","value":" 7.55k/? [00:00&lt;00:00, 254kB/s]"}},"83f0deeb71a843288ba08857705bd7a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70a86d482a6646619f3a4c7216e63af3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f14793a27b834b059c6ad6b92c01b811":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54d7ec07dfb84552b41aad7293cf2e21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc862120d1ee4a0383db0f7913b3946e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9017665d4c2640f9b978c26537a4e892":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf9f8d97ecde4468999216e90f88c086":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58ab411aa72e4e0caad2a68259a7fdd7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_71e37e15efad4fa698dfed8713d5ad03","IPY_MODEL_2f31eb4aee6e434dbee98fd13de7e567","IPY_MODEL_6683d6ca39b842bd94d066b9844798df"],"layout":"IPY_MODEL_3fef1eccee54437ebcabc3d9922f18cc"}},"71e37e15efad4fa698dfed8713d5ad03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9ab82f3187e47ec93ff380010257266","placeholder":"​","style":"IPY_MODEL_42ec3df33ee74bbba61d7928368cbbb4","value":"Downloading builder script: "}},"2f31eb4aee6e434dbee98fd13de7e567":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16d9cf636d554c209546f6556c444db8","max":2524,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a2d2884069f43bb94b9576aca442efb","value":2524}},"6683d6ca39b842bd94d066b9844798df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bbabc5dacb34b3d9aeae0314cc50b5b","placeholder":"​","style":"IPY_MODEL_85772050819340138e74f75e1e6c553c","value":" 7.38k/? [00:00&lt;00:00, 250kB/s]"}},"3fef1eccee54437ebcabc3d9922f18cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9ab82f3187e47ec93ff380010257266":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42ec3df33ee74bbba61d7928368cbbb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16d9cf636d554c209546f6556c444db8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a2d2884069f43bb94b9576aca442efb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8bbabc5dacb34b3d9aeae0314cc50b5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85772050819340138e74f75e1e6c553c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8fae328cb4740788f879b90fdb07b0d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32926dba443e4ecba1e6c919e51ab282","IPY_MODEL_67c2581280ac4d3eba33740a59dc0a20","IPY_MODEL_c8baad3154dd425f8232d25057f36590"],"layout":"IPY_MODEL_1185ce4988da4557b553e8f9c00e8c07"}},"32926dba443e4ecba1e6c919e51ab282":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56deec79b6fb4a4cb12e78784040bc07","placeholder":"​","style":"IPY_MODEL_69f5595882b44d4e8ad6d18d7ee36ef8","value":"Downloading builder script: "}},"67c2581280ac4d3eba33740a59dc0a20":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e412b5e5e69a46b397ba218e455b6ad7","max":2318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_285d0a71fe39437cbd72108aed52c83a","value":2318}},"c8baad3154dd425f8232d25057f36590":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e26343c18d0342119f49f49e0bd8ec41","placeholder":"​","style":"IPY_MODEL_2f43a513bcc44691b84a17a2b42f1ab2","value":" 6.50k/? [00:00&lt;00:00, 236kB/s]"}},"1185ce4988da4557b553e8f9c00e8c07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56deec79b6fb4a4cb12e78784040bc07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69f5595882b44d4e8ad6d18d7ee36ef8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e412b5e5e69a46b397ba218e455b6ad7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"285d0a71fe39437cbd72108aed52c83a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e26343c18d0342119f49f49e0bd8ec41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f43a513bcc44691b84a17a2b42f1ab2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}